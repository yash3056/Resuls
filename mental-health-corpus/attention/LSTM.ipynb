{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"999aaf37-c727-4c1b-8151-7fe2df254c75","_cell_guid":"e9be33da-844c-4110-8c3f-a71bb7b55e91","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:31.961217Z","iopub.execute_input":"2024-08-12T15:24:31.961602Z","iopub.status.idle":"2024-08-12T15:24:33.135185Z","shell.execute_reply.started":"2024-08-12T15:24:31.961568Z","shell.execute_reply":"2024-08-12T15:24:33.133815Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mental-health-corpus/mental_health.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport string\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom torchtext.vocab import build_vocab_from_iterator","metadata":{"_uuid":"d3afe6ca-447d-4809-b0c7-7633b55991db","_cell_guid":"0f636fa6-e0c3-4187-abd2-d4141cc02386","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:33.137260Z","iopub.execute_input":"2024-08-12T15:24:33.137764Z","iopub.status.idle":"2024-08-12T15:24:39.520409Z","shell.execute_reply.started":"2024-08-12T15:24:33.137728Z","shell.execute_reply":"2024-08-12T15:24:39.519221Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation_and_emojis(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Remove emojis\n    emoji_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n        u\"\\U0001FA00-\\U0001FA6F\"  # Extended Symbols and Pictographs\n        u\"\\U0001FAB0-\\U0001FABF\"  # Additional animals & nature\n        u\"\\U0001FAC0-\\U0001FAFF\"  # Additional people & body\n        u\"\\U0001FAD0-\\U0001FAFF\"  # Additional food & drink\n        u\"\\U00002000-\\U0000206F\"  # General Punctuation (includes â€¼)\n        u\"\\U00002300-\\U000023FF\"  # Miscellaneous Technical\n        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n        \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    return text\n\ndef process_text(text):\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation and emojis\n    text = remove_punctuation_and_emojis(text)\n\n    return text","metadata":{"_uuid":"e7472282-fad9-4a3b-8537-e4e0bc364b52","_cell_guid":"b2233559-5b2a-43c5-918e-3915bbcd8131","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:39.521767Z","iopub.execute_input":"2024-08-12T15:24:39.522353Z","iopub.status.idle":"2024-08-12T15:24:39.530864Z","shell.execute_reply.started":"2024-08-12T15:24:39.522318Z","shell.execute_reply":"2024-08-12T15:24:39.529604Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mental-health-corpus/mental_health.csv')\ndf['text'] = df['text'].apply(lambda x: process_text(str(x)))\ndf.head()","metadata":{"_uuid":"33ea452e-c7c0-4b93-a285-bb961a8a9dfb","_cell_guid":"fc6ba930-9c12-4052-bbf9-9c1f0c6d4c64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:39.533418Z","iopub.execute_input":"2024-08-12T15:24:39.533796Z","iopub.status.idle":"2024-08-12T15:24:40.676826Z","shell.execute_reply.started":"2024-08-12T15:24:39.533764Z","shell.execute_reply":"2024-08-12T15:24:40.675516Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  dear american teens question dutch person hear...      0\n1  nothing look forward lifei dont many reasons k...      1\n2  music recommendations im looking expand playli...      0\n3  im done trying feel betterthe reason im still ...      1\n4  worried  year old girl subject domestic physic...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dear american teens question dutch person hear...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nothing look forward lifei dont many reasons k...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>music recommendations im looking expand playli...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>im done trying feel betterthe reason im still ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>worried  year old girl subject domestic physic...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Create label mapping\nunique_labels = df['label'].unique()\nlabel_dict = {label: i for i, label in enumerate(unique_labels)}\nnum_labels = len(unique_labels)\nprint(\"Label mapping:\", label_dict)","metadata":{"_uuid":"4b31edf6-207b-417d-8d8f-96ba329a7a6a","_cell_guid":"009624d5-e81c-4321-8152-7c9cc55f54ea","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:40.678247Z","iopub.execute_input":"2024-08-12T15:24:40.678585Z","iopub.status.idle":"2024-08-12T15:24:40.690390Z","shell.execute_reply.started":"2024-08-12T15:24:40.678556Z","shell.execute_reply":"2024-08-12T15:24:40.688789Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Label mapping: {0: 0, 1: 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Build vocabulary\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        yield text.split()\n\nvocab = build_vocab_from_iterator(yield_tokens(df['text']), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-12T15:24:40.691994Z","iopub.execute_input":"2024-08-12T15:24:40.692415Z","iopub.status.idle":"2024-08-12T15:24:41.543698Z","shell.execute_reply.started":"2024-08-12T15:24:40.692384Z","shell.execute_reply":"2024-08-12T15:24:41.542480Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# LSTM Model\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        # lstm_output shape: (batch_size, seq_len, hidden_dim)\n        attention_weights = F.softmax(self.attention(lstm_output), dim=1)\n        # attention_weights shape: (batch_size, seq_len, 1)\n        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n        # context_vector shape: (batch_size, hidden_dim)\n        return context_vector, attention_weights\n\nclass AttentionLSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n        self.attention = Attention(hidden_dim * 2 if bidirectional else hidden_dim)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):\n        # text shape: (batch_size, seq_len)\n        embedded = self.dropout(self.embedding(text))\n        # embedded shape: (batch_size, seq_len, embedding_dim)\n        \n        lstm_output, (hidden, cell) = self.lstm(embedded)\n        # lstm_output shape: (batch_size, seq_len, hidden_dim * num_directions)\n        \n        context_vector, attention_weights = self.attention(lstm_output)\n        # context_vector shape: (batch_size, hidden_dim * num_directions)\n        \n        return self.fc(self.dropout(context_vector))","metadata":{"_uuid":"0b0c33a3-b04e-45cb-8e43-413617451a4b","_cell_guid":"502d6cd4-1ebf-4c30-8ad6-695ae8db6c61","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:41.545246Z","iopub.execute_input":"2024-08-12T15:24:41.545721Z","iopub.status.idle":"2024-08-12T15:24:41.557054Z","shell.execute_reply.started":"2024-08-12T15:24:41.545687Z","shell.execute_reply":"2024-08-12T15:24:41.555703Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, vocab, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        # Convert text to tensor of indices\n        numericalized_text = [self.vocab[token] for token in text.split()]\n        if len(numericalized_text) < self.max_len:\n            numericalized_text += [self.vocab[\"<unk>\"]] * (self.max_len - len(numericalized_text))\n        else:\n            numericalized_text = numericalized_text[:self.max_len]\n        \n        return torch.tensor(numericalized_text), torch.tensor(label)","metadata":{"_uuid":"41b66e33-b874-4903-8ac1-9acf3382efe7","_cell_guid":"65525dbb-fd5a-4bbe-a212-8b6d70d94d37","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:41.558728Z","iopub.execute_input":"2024-08-12T15:24:41.559127Z","iopub.status.idle":"2024-08-12T15:24:41.576150Z","shell.execute_reply.started":"2024-08-12T15:24:41.559081Z","shell.execute_reply":"2024-08-12T15:24:41.574861Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Prepare data\nmax_len = 128  # or any other suitable length\nX_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'].map(label_dict), test_size=0.2, random_state=42)\n\ntrain_dataset = CustomDataset(X_train.tolist(), y_train.tolist(), vocab, max_len)\nval_dataset = CustomDataset(X_val.tolist(), y_val.tolist(), vocab, max_len)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)","metadata":{"_uuid":"0a1404ba-357c-40dc-8796-a7c87c08ea80","_cell_guid":"60bbbf8d-8946-4f4f-af46-f0aa679285de","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:41.577684Z","iopub.execute_input":"2024-08-12T15:24:41.578131Z","iopub.status.idle":"2024-08-12T15:24:41.609709Z","shell.execute_reply.started":"2024-08-12T15:24:41.578071Z","shell.execute_reply":"2024-08-12T15:24:41.608221Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Initialize model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvocab_size = len(vocab)\nembedding_dim = 100\nhidden_dim = 256\noutput_dim = num_labels\nn_layers = 2\nbidirectional = False\ndropout = 0.5\n\nmodel = AttentionLSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\nmodel = nn.DataParallel(model, device_ids=[0, 1])  # Enable multi-GPU support\nmodel = model.to(device)\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-12T15:24:41.612458Z","iopub.execute_input":"2024-08-12T15:24:41.612840Z","iopub.status.idle":"2024-08-12T15:24:41.752888Z","shell.execute_reply.started":"2024-08-12T15:24:41.612809Z","shell.execute_reply":"2024-08-12T15:24:41.751484Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training loop\noptimizer = Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    for batch_text, batch_labels in train_loader:\n        batch_text, batch_labels = batch_text.to(device), batch_labels.to(device)\n        \n        optimizer.zero_grad()\n        predictions = model(batch_text)\n        loss = criterion(predictions, batch_labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n    \n    model.eval()\n    total_val_loss = 0\n    all_predictions = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch_text, batch_labels in val_loader:\n            batch_text, batch_labels = batch_text.to(device), batch_labels.to(device)\n            \n            predictions = model(batch_text)\n            loss = criterion(predictions, batch_labels)\n            total_val_loss += loss.item()\n            \n            _, predicted = torch.max(predictions, 1)\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(batch_labels.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions, average='weighted')\n    recall = recall_score(all_labels, all_predictions, average='weighted')\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Training Loss: {total_train_loss/len(train_loader):.4f}\")\n    print(f\"Validation Loss: {total_val_loss/len(val_loader):.4f}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"--------------------\")","metadata":{"_uuid":"f31c785d-a595-4cc6-b908-09fc1d9c87e4","_cell_guid":"19e530d5-1dae-4abb-9149-8ef7a53103d6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-12T15:24:41.754308Z","iopub.execute_input":"2024-08-12T15:24:41.754640Z","iopub.status.idle":"2024-08-12T16:08:48.249230Z","shell.execute_reply.started":"2024-08-12T15:24:41.754612Z","shell.execute_reply":"2024-08-12T16:08:48.247055Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/10\nTraining Loss: 0.4245\nValidation Loss: 0.2951\nAccuracy: 0.8797\nPrecision: 0.8797\nRecall: 0.8797\nF1 Score: 0.8797\n--------------------\nEpoch 2/10\nTraining Loss: 0.3004\nValidation Loss: 0.3182\nAccuracy: 0.8735\nPrecision: 0.8834\nRecall: 0.8735\nF1 Score: 0.8727\n--------------------\nEpoch 3/10\nTraining Loss: 0.2551\nValidation Loss: 0.2523\nAccuracy: 0.9039\nPrecision: 0.9069\nRecall: 0.9039\nF1 Score: 0.9037\n--------------------\nEpoch 4/10\nTraining Loss: 0.2337\nValidation Loss: 0.2216\nAccuracy: 0.9137\nPrecision: 0.9140\nRecall: 0.9137\nF1 Score: 0.9137\n--------------------\nEpoch 5/10\nTraining Loss: 0.2146\nValidation Loss: 0.2552\nAccuracy: 0.9083\nPrecision: 0.9109\nRecall: 0.9083\nF1 Score: 0.9082\n--------------------\nEpoch 6/10\nTraining Loss: 0.1962\nValidation Loss: 0.2411\nAccuracy: 0.9110\nPrecision: 0.9145\nRecall: 0.9110\nF1 Score: 0.9108\n--------------------\nEpoch 7/10\nTraining Loss: 0.1810\nValidation Loss: 0.2283\nAccuracy: 0.9128\nPrecision: 0.9148\nRecall: 0.9128\nF1 Score: 0.9127\n--------------------\nEpoch 8/10\nTraining Loss: 0.1654\nValidation Loss: 0.2532\nAccuracy: 0.9105\nPrecision: 0.9141\nRecall: 0.9105\nF1 Score: 0.9103\n--------------------\nEpoch 9/10\nTraining Loss: 0.1556\nValidation Loss: 0.2200\nAccuracy: 0.9203\nPrecision: 0.9203\nRecall: 0.9203\nF1 Score: 0.9203\n--------------------\nEpoch 10/10\nTraining Loss: 0.1413\nValidation Loss: 0.2351\nAccuracy: 0.9214\nPrecision: 0.9221\nRecall: 0.9214\nF1 Score: 0.9213\n--------------------\n","output_type":"stream"}]}]}